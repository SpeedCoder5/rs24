name: llm-service
ray_serve_config:
  applications:
  - args:
      models: []
      vllm_base_models:
        # Link to list of models you want to serve here. You can add more than one model
        - "./models/llama/meta-llama--Llama-2-7b-chat-hf_a10g_tp1.yaml" # replace with models/llama/meta-llama--Llama-2-7b-chat-hf_l4_tp1.yaml for GCE
    runtime_env:
      env_vars:
        HUGGING_FACE_HUB_TOKEN: "UPDATE HF TOKEN HERE"
    import_path: aviary_private_endpoints.backend.server.run:router_application
    route_prefix: /
    name: llm-endpoint
